{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Fares Dataset - Select and Training Models \n",
    "In this fourth notebook, we have two aims:\n",
    "1) Choose some metrics to evaluate the model performance;\n",
    "2) Select a set of models and test them in our training data. Once we have the best ones, we will use them to make predictions on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# scikit-learn libraries \n",
    "from sklearn.model_selection import cross_val_score # cross-validation\n",
    "from sklearn.model_selection import GridSearchCV # gridsearch CV \n",
    "from sklearn.linear_model import LinearRegression # linear regression\n",
    "from sklearn.neighbors import KNeighborsRegressor # KNN for regression \n",
    "from sklearn.tree import DecisionTreeRegressor # basic decision tree regression \n",
    "from sklearn.ensemble import RandomForestRegressor # random forest regression \n",
    "from sklearn.metrics import mean_squared_error # mean squared error is the metric to be used \n",
    "\n",
    "# xgboost and lightgbm \n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "\n",
    "# joblib and pickle to save models\n",
    "import joblib\n",
    "\n",
    "# remove future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='error', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../../uber-fares-prediction/data/processed/'\n",
    "\n",
    "# prepared training set \n",
    "X_train_prepared = (\n",
    "    pd.read_csv(\n",
    "        root_path + 'uber_prepared_train_set.csv'\n",
    "    )\n",
    ")\n",
    "\n",
    "# prepared validation set \n",
    "X_test_prepared = (\n",
    "    pd.read_csv(\n",
    "        root_path + 'uber_prepared_validation_set.csv'\n",
    "    )\n",
    ")\n",
    "\n",
    "# target validation set \n",
    "y_train = (\n",
    "    pd.read_csv(\n",
    "        root_path + 'uber_validation_target.csv'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into an array\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Lot of Models using Cross-Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are studying a regression problem, the most common metric to this class of problems is the **Mean Squared Error**:\n",
    "$$\\textrm{MSE}(\\textbf{X}, h)=\\frac{1}{N}\\sum_{i=1}^{N}\\left(y^{(i)}-h(\\textbf{x}^{(i)})\\right)^2,$$\n",
    "where $h(\\textbf{x}^{(i)})$ is the prediction of the model $h$ for the example $\\textbf{x}^{(i)}$ of our data, and $y^{(i)}$ is the true label for this example. Beyond it, we will also work with the of MSE:\n",
    "$$\\textrm{RMSE}(\\textbf{X}, h)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}\\left(y^{(i)}-h(\\textbf{x}^{(i)})\\right)^2},$$\n",
    "which is **Root Mean Squared Error (RMSE)**. Other two important metrics for regression problems are **Mean Absolute Error**:\n",
    "$$\\textrm{MAE}(\\textbf{X}, h)=\\frac{1}{N}\\sum_{i=1}^{N}\\left|y^{(i)}-h(\\textbf{x}^{(i)})\\right|,$$\n",
    "and $R^2$:\n",
    "$$R^{2}(h)=1-\\frac{\\sum_{i=1}^{N}\\left(y^{(i)}-h(\\textbf{x}^{(i)})\\right)^2}{\\sum_{i=1}^{N}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^2}=1-\\frac{\\textrm{MSE}(h)}{\\textrm{MSE}(\\bar{y})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to select a set of models of different types and testing them into the validation set. Then, after we have selected the best ones (or the best one), we will fine-tunning our model to make better predictions. Finally, the last goal is to apply the model to the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the following models:\n",
    "1) Linear Regression (LR);\n",
    "2) K-Nearest Neighbors Regression (KNN);\n",
    "3) Decision Tree Regression (DTR);\n",
    "4) Random Forest Regression (RFR);\n",
    "5) XGBoost for Regression (XGBR);\n",
    "6) LightGBM for Regression (LGBR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate all models using default hyperparameters and create a list of these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating all models \n",
    "lin_reg = LinearRegression() # Linear regression\n",
    "knn_reg = KNeighborsRegressor() # knn regression \n",
    "tree_reg = DecisionTreeRegressor() # Decision Tree Regressor - the criterion to split is squared_error by default \n",
    "forest_reg = RandomForestRegressor() # Random Forest Regressor - the number of estimators is 100 by default \n",
    "xgb_reg = xgb.XGBRegressor() # XGBoost Regressor \n",
    "lgb_reg = lgb.LGBMRegressor() # LightGBM Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_classes = {\n",
    "    'LR': lin_reg,\n",
    "    'KNN': knn_reg,\n",
    "    'DTR': tree_reg,\n",
    "    'RFR': forest_reg,\n",
    "    'XGBR': xgb_reg,\n",
    "    'LGBR': lgb_reg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 8.649837 (1.499849)\n",
      "KNN: 10.258877 (0.213912)\n",
      "DTR: 6.099226 (0.440399)\n",
      "RFR: 4.317930 (0.358635)\n",
      "XGBR: 4.248058 (0.345017)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1356\n",
      "[LightGBM] [Info] Number of data points in the train set: 91560, number of used features: 10\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1356\n",
      "[LightGBM] [Info] Start training from score 11.335618\n",
      "[LightGBM] [Info] Number of data points in the train set: 91560, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 11.375572\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1356\n",
      "[LightGBM] [Info] Number of data points in the train set: 91560, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 11.338605\n",
      "LGBR: 4.337817 (0.393018)\n"
     ]
    }
   ],
   "source": [
    "# evaluating each model in turn \n",
    "results = []\n",
    "names = []\n",
    "for name, model in models_dict_classes.items(): \n",
    "    cv_results = cross_val_score(\n",
    "        model, \n",
    "        X_train_prepared, \n",
    "        y_train,\n",
    "        cv=3,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    results.append(np.sqrt(-cv_results))\n",
    "    names.append(name)\n",
    "    final_results = dict(zip(names, results))\n",
    "    print('%s: %f (%f)' % (name, np.sqrt(-cv_results).mean(), np.sqrt(-cv_results).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to evaluate the performance by using RMSE as a standard metric. Of course, RMSE alone cannot say all. In a complete analysis, it is important to observe other metrics like R2 and MAE, for example - we will do that when evaluating the model in our test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results, using RMSE, for each model are:\n",
    "1) Logistic Regression: $8.649837 \\pm 1.499849$;\n",
    "2) kNN Regression: $10.258877 \\pm 0.213912$;\n",
    "3) Decision Tree Regression: $6.099226 \\pm 0.440399$;\n",
    "4) Random Forest Regression: $4.317930 \\pm 0.358635$;\n",
    "5) XGBoost Regression: $4.248058 \\pm 0.345017$;\n",
    "6) LGBM Regression: $4.337817 \\pm 0.393018$\n",
    "\n",
    "We can see the three best models are Random Forest Regression, XGBoost Regression and LGBM Regression. Then, we will maintain them to apply in our unseen data and tunning hyperparameters to obtain the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save the three best vanilla models as pickle files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/interim/random_forest_regression.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving Random Forest Regression model\n",
    "random_forest_model_path = '../models/interim/random_forest_regression.pkl'\n",
    "joblib.dump(forest_reg, random_forest_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/interim/xgboost_regression.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving XGBoost regression model \n",
    "xgb_reg_model_path = '../models/interim/xgboost_regression.pkl'\n",
    "joblib.dump(xgb_reg, xgb_reg_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/interim/lgbm_regression.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving lgbm regression model \n",
    "lgb_reg_path = '../models/interim/lgbm_regression.pkl'\n",
    "joblib.dump(lgb_reg, lgb_reg_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
